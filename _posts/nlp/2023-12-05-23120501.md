---
title: "[논문리뷰] Attention Is All You Need 번역"
excerpt: "어텐션(Attention) 기법과 트랜스포머(Transformer)"

categories:
  - NLP

toc: false
toc_sticky: false

date: 2023-12-05
last_modified_at: 2023-12-14
---

> '[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)' 논문을 번역한 글입니다.  

# Attention Is All You Need

## Abstract

주요 문장 번역 모델들은 인코더와 디코더를 포함하는 복잡한 RNN 또는 CNN에 기반한다. 최고 성능의 모델은 어텐션 매커니즘을 통해 인코더와 디코더를 연결한다. 트랜스포머(Transformer)는 순환과 합성곱(convolutions)을 제외하고 어텐션(Attention) 메커니즘만 사용하는 간단한 네트워크 아키텍처이다. 두 가지 기계 번역 작업에 대한 실험 결과, 이 모델들은 품질이 우수할 뿐만 아니라 병렬 처리가 가능하고 훈련 시간도 현저히 짧다는 것을 보여준다. 트랜스포머 모델은 WMT 2014 영어-독일어 번역 작업에서 기존 최고 결과보다 2BLEU 이상 향상된 28.4 BLEU를 달성했다. WMT 2014 영어-프랑스어 번역 작업에서 8개의 GPU에서 3.5일 동안 학습한 뒤 SOTA(41.8 BLEU)를 달성했으며, 이는 기존 최고 모델보다 훈련 비용이 훨씬 적다. 트랜스포머를 대규모 및 제한된 훈련 데이터를 사용한 영어 구문 분석 작업에 성공적으로 적용시켜 트랜스포머가 다른 작업들에도 잘 일반화된다는 것을 보여준다.  

## 1. Introduction

RNN, 특히 LSTM과 GRU는 언어 모델링, 기계 번역과 같은 시퀀스 모델링과 변환 문제에서 SOTA로 확고히 자리잡았다. 순환 언어 모델과 인코더-디코더 아키텍처의 경계를 넓히기 위한 많은 노력이 계속되고 있다.  

순환 모델은 일반적으로 계산 과정을 입력 및 출력 시퀀스의 심볼 위치에 따라 나눈다. 계산 시간의 단계와 위치를 맞추면서, 순환 모델은 이전 은닉 상태 $h_{t-1}$과 위치 $t$의 입력으로 은닉 상태 $h_t$ 시퀀스를 생성한다. 이러한 본질적으로 순차적인 특성은 훈련 예제 내에서 병렬 처리를 방해하는데, 이는 긴 시퀀스 길이에서 중요해지며, 기억장치 제약으로 인해 예제 간 배치가 제한됨에 따라 더욱 그러하다. 최근 연구는 인수 분해 기법(factorization tricks)과 조건부 계산(conditional computation)을 통해 계산 효율성을 크게 향상시켰고, 조건부 계산에서는 모델 성능 또한 향상시켰다. 그러나 순차적인 계산의 근본적인 제약은 여전히 남아있다.  

어텐션 메커니즘은 다양한 작업에서 시퀀스 모델링과 변환 모델의 중요한 부분이 되었고, 거리와 입력 또는 출력 시퀀스에 상관 없이 의존성을 모델링할 수 있게한다. 그러나 몇가지 경우를 제외하고, 어텐션 메커니즘은 순환 네트워크와 함께 사용된다.  

이 작업에서 입출력 사이의 전역 의존성을 그리는 어텐션 메커니즘에만 의존하고 순환을 피하는 모델 아키텍처인 트랜스포머를 제안한다. 트랜스포머는 상당히 더 많은 병렬화를 허용하고, 8개의 P100 GPU에서 12시간만큼 적은 학습 후 번역 품질에서 SOTA를 달성할 수 있다.  

## 2. Background

순차적인 계산을 줄이는 목표는 또한 Extended Neural GPU, ByteNet, ConvS2S의 기초를 형성하고, 이들 모두 모든 입출력 위치에서 은닉 표현들을 병렬적으로 계산하는 기본 구성요소로써 CNN을 사용한다.  이 모델에서 두 임의의 입출력 위치로부터 신호를 연결하기 위해 필요한 작업의 수는 ConvS2S에서는 선형적이고 ByteNet에서는 대수적으로 위치들 사이의 거리에 따라 증가한다.  이것은 먼 위치 사이의 의존성을 학습하는 것을 더 어렵게 한다. 트랜스포머에서 이는 작업의 일정 수를 줄이고, 심지어 어텐션 가중 위치의 평균화로 감소된 유효 해결책의 비용에서, 3.2 섹션에 설명된 Multi-Head Attention 으로 상쇄하는 효과이다.  

self-attention(intra-attention)은 시퀀스의 표현을 계산하기 위해 단일 시퀀스 내의 다른 위치들을 관련짓는 어텐션 메커니즘이다. 셀프 어텐션(self-attention)은 독해, 추상적 요약, 텍스트 추론(entailment) 및 작업 독립적인 문장 표현 학습과 같은 다양한 작업에서 성공적으로 사용되어왔다.  
(* entailment: "함의" 또는 "추론"으로 해석될 수 있습니다. 이 맥락에서, "textual entailment"는 텍스트 간의 논리적 관계를 파악하는 과정을 의미합니다. 즉, 하나의 텍스트 조각(전제)이 다른 텍스트 조각(결론)을 논리적으로 수반하거나 암시하는지를 분석하는 것입니다.)  

End-to-end memory networks는 시퀀스 정렬 재귀 대신에 순환 어텐션 메커니즘에 기반하고, 간단한 언어 질의 응답과 언어 모델링 직업에서 잘 수행되는 것으로 나타났다.  

그러나 우리가 아는 한, 트랜스포머는 시퀀스 정렬된 RNN 또는 합성곱 사용 없이 입출력 표현을 계산하기 위해 셀프 어텐션에 전적으로 의존하는 첫 변환 모델이다. 아래 섹션들에 따라, 우리는 셀프 어텐션에 대해 설명하고, 17, 18, 9와 같은 모델들에 비해 이것의 장점을 논의하며 트랜스포머에 대해 설명할 것이다.  

## 3. Model Architecture

가장 경쟁력있는 신경망 시퀀스 변환 모델은 인코더-디코더 구조를 가진다. 여기서, 인코더는 심볼 표현의 입력 시퀀스 $(x_1, \cdots, x_n)$를 연속적인 표현의 시퀀스 $\mathbf{z} = (z_1, \cdots, z_n)$로 매핑한다. $\mathbf{z}$ 가 주어지면, 디코더는 한 번에 하나의 요소로 출력 시퀀스 $(y_1, \cdots, y_m)$ 를 생성한다. 각 단계에서 모델은 자동 회귀적으로, 다음 심볼을 생성할 때 추가적인 입력으로써 이전에 생성된 심볼을 사용한다.  

![Figure1](/assets/images/23120501/Figure1.png)  

트랜스포머는 Figure 1의 각 왼쪽 및 오른쪽 절반에서 보여지는 쌓인 self-attention과 인코더와 디코더 모두에 대해 완전 연결 계층인 지점 별로 사용된 전체 아키텍처를 따른다.  

### 3.1. Encoder and Decoder Stacks

**Encoder:** 인코더는 $N = 6$ 개의 동일한 레이어의 스택으로 구성된다. 각 레이어는 두 개의 서브 레이어를 가진다. 첫번째는 multi-head self-attention 메커니즘이고, 두번째는 간단한 위치 별 완전 연결 피드 포워드(feed-forward) 네트워크이다. 우리는 두 개의 서브 레이어 각각의 주변에 잔차 연결(residual connection)을 사용하고, 그 다음에 레이어 정규화를 수행했다. 즉, 각 서브 레이어의 출력은 $\mathrm{LayerNorm}(x+\mathrm{Sublayer}(x))$이고, $\mathrm{Sublayer}(x)$는 서브 레이어 자체에 의해 구현된 함수이다. 이 잔차 연결이 가능하게 하기 위해, 모델의 모든 서브 레이어와 임베딩 레이어는 $d_{\mathrm{model}}=512$의 차원을 가진 출력을 생산한다.  

**Decoder:** 디코더 또한 $N = 6$ 개의 동일한 레이어의 스택으로 구성된다. 각 인코더 레이어의 두 개의 서브 레이어 외에도, 디코더는 인코더 스택의 출력에 멀티 헤드(multi-head) 어텐션을 수행하는 세번째 서브 레이어를 삽입한다. 인코더와 비슷하게, 우리는 각 서브 레이어 주변에 잔차 연결을 사용하고, 레이어 정규화를 수행했다. 우리는 또한 위치들이 후속 위치들을 고려하는 것을 막기 위해 디코더 스택의 셀프 어텐션 서브 레이어를 수정한다. 출력 임베딩이 하나의 위치로부터의 오프셋이라는 사실을 포함한 이 마스킹은, 위치 $i$로부터의 예측이 $i$보다 적은 위치의 알려진 출력에만 의존할 수 있게 한다.  

### 3.2. Attention

![Figure2](/assets/images/23120501/Figure2.png)  

Figure2: (왼쪽) Scaled Dot-Product Attention. (오른쪽) 멀티 헤드 어텐션은 병렬로 실행되는 여러 어텐션 레이어로 구성된다.  

어텐션 함수는 쿼리와 키값 쌍의 집합을 출력으로 매핑하는 것으로 설명될 수 있으며, 이때 쿼리, 키, 값 그리고 출력은 모두 벡터이다. 출력은 값의 가중된 합으로 계산되며, 이때 각 값에 할당된 가중치는 해당 키와 함께 쿼리의 호환성 함수로 계산된다.  

#### 3.2.1. Scaled Dot-Product Attention

우리는 우리의 특별한 어텐션을 Scaled Dot-Product Attention이라고 부른다. 입력은 차원 $d_k$의 쿼리들과 키들, 그리고 차원 $d_v$의 값들로 구성된다. 우리는 모든 키들과 함께 쿼리의 닷 프로덕트(dot products)를 계산하고, 각각을 $\sqrt{d_k}$로 나누고, 값에 대한 가중치를 얻기 위해 소프트맥스 함수를 적용한다.  

실제로, 우리는 행렬 $Q$로 묶인 쿼리 세트에 대해 동시에 어텐션 함수를 계산한다. 키와 값들은 또한 행렬 $K$와 $V$로 묶인다. 우리는 출력 행렬을 다음과 같이 계산한다:  

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

가장 흔하게 사용되는 두 개의 어텐션 함수는 가산 어텐션(additive attention)과 닷 프로덕트(곱셈) 어텐션(dot-product attention)이다. 닷 프로덕트 어텐션은 $\frac{1}{\sqrt{d_k}}$의 스케일링 인자를 제외하면 우리의 알고리즘과 동일하다. 가산 어텐션은 단일 은닉 층과 함께 피드 포워드 네트워크를 사용해서 호환성 함수를 계산한다. 두 개가 이론적 복잡성에서 비슷한 반면, 닷 프로덕트 어텐션은 고도로 최적화된 행렬 곱셈 코드를 사용하여 구현될 수 있기 때문에 실제로 더 빠르고 공간 효율적이다.  

$d_k$의 작은 값들에 대해 두 메커니즘은 비슷하게 수행되지만, 가산 어텐션은 $d_k$의 더 큰 값에 대한 스케일링이 없는 닷 프로덕트 어텐션을 능가한다. 우리는 $d_k$의 큰 값들에 대해, 닷 프로덕트의 크기가 커져서 소프트맥스 함수를 극도로 작은 기울기를 가진 영역으로 밀어내기 때문이라고 추측한다. 이러한 효과를 거스르기 위해, 우리는 닷 프로덕트를 $\frac{1}{\sqrt{d_k}}$로 스케일링한다.  

#### 3.2.2. Multi-Head Attention

$d_{\text{model}}$ 차원의 키, 값과 쿼리들과 함께 단일 어텐션 함수를 수행하는 것 대신, 우리는 $d_k$, $d_k$ 와 $d_v$ 차원 각각에 선형 투영(projections)을 학습한 다른 쿼리, 키, 값들을 $h$번 선형적으로 투영하는 것이 유익하다는 것을 발견했다. 쿼리, 키, 값들의 이 투영 버전 각각에서 우리는 병렬적으로  어텐션 함수를 수행하고, $d_v$ 차원적인 출력 값을 산출한다. 이들은 연결되고 다시 한 번 투영되고, Figure 2에 묘사된 최종 값들은 결과가 된다.  

멀티 헤드 어텐션은 모델이 다른 위치에서 다른 표현 부분공간으로부터의 정보에 공동으로 주목할 수 있게 한다. 단일 어텐션 헤드에서, 평균화는 이를 억제한다.  

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{where } \text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
$$

여기서 투영은 매개변수 행렬 $ W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k} $, $ W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k} $, $ W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v} $, $ W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}} $에 속한다.  

이 작업에서 우리는 $h = 8$개의 병렬 어텐션 레이어, 즉 헤드를 사용한다. 이들 각각에서 우리는 $d_k = d_v = d_{\text{model}}/h = 64$를 사용한다. 각 헤드에서 감소된 차원 덕분에, 전체 계산 비용은 전체 차원을 가진 단일 헤드 어텐션의 비용과 비슷하다.  

#### 3.2.3. Applications of Attention in our Model

트랜스포머는 3가지 다른 방식의 멀티 헤드 어텐션을 사용한다.  

- 인코더-디코더 어텐션 레이어에서, 쿼리들은 이전 디코더 레이어로부터 오고, 메모리 키와 값들은 인코더의 출력으로부터 온다. 이것은 디코더의 모든 위치들이 입력 시퀀스의 모든 위치들을 주목할 수 있게 한다. 이것은 [38, 2, 9]과 같이 sequence-to-sequence 모델의 전형적인 인코더-디코더 어텐션 메커니즘을 모방한다.  
- 인코더는 셀프 어텐션 레이어를 포함한다. 셀프 어텐션 레이어에서 키, 값, 쿼리들의 모든 것은 같은 공간으로부터 오고, 이 경우, 인코더의 이전 레이어의 출력에서 온다. 인코더의 각 위치는 인코더의 이전 레이어에서 모든 위치들에 주목할 수 있다.  
- 마찬가지로, 디코더의 셀프 어텐션 레이어는 디코더의 각 위치가 해당 위치를 포함해 디코더의 모든 위치까지 주목할 수 있게 한다. 우리는 자동 회귀 특성을 보존하기 위해 디코더의 왼쪽으로부터의 정보 흐름을 막는 것이 필요하다. 우리는 스케일링된 닷 프로덕트 어텐션 내부에서 불법적인 연결에 대응하는 소프트맥스의 입력에서 모든 값을 마스킹(−∞로 설정)함으로써 이것을 구현한다. Figure 2 참조.  

### 3.3. Position-wise Feed-Forward Networks

게다가 인코더와 디코더의 각 레이어인 어텐션 서브 레이어는 각 위치에 별도로 동일하게 적용되는 완전 연결 피드 포워드 네트워크를 포함한다. 이것은 사이에 ReLU 활성화 함수가 있는 두개의 선형 변환(linear transformation)으로 구성된다.  

$$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$

선형 변환은 서로 다른 위치에서 동일한 반면, 그들은 레이어마다 다른 매개변수를 사용한다. 이를 설명하는 다른 방법은 커널 크기가 1인 두 개의 합성곱이다. 입출력의 차원은 $d_{\text{model}} = 512$이고, 내부 레이어(inner-layer)는 차원 $d_{ff} = 2048$를 가진다.  

### 3.4. Embeddings and Softmax

다른 시퀀스 변환 모델과 마찬가지로, 우리는 입력 토큰과 출력 토큰을 차원 $d_{\text{model}}$의 벡터로 변환하는 학습된 임베딩을 사용한다. 우리는 또한 디코더 출력을 예측된 다음 토큰 확률로 변환하기 위해 일반적인 학습된 선형 변환과 소프트맥스(softmax) 함수를 사용한다. 우리의 모델에서, 우리는 [30]과 같이 두 개의 임베딩 레이어와 사전 소프트맥스(pre-softmax) 선형 변환 사이에 같은 가중치 행렬을 공유한다. 임베딩 레이어에서 우리는 이 가중치에 $\sqrt{d_{\text{model}}}$을 곱한다.  

Tabel 1: 다른 레이어 유형에 대한 최대 경로 길이, 레이어 별 복잡성과  최소 연속 작업의 수. $n$은 시퀀스 길이, $d$는 표현 차원, $k$는 합성곱의 커널 크기, $r$은 제한된 셀프 어텐션의 이웃(neighborhood) 크기.  

| Layer Type                       | Complexity per Layer        | Sequential Operations | Maximum Path Length  |
|----------------------------------|-----------------------------|-----------------------|----------------------|
| $ \text{Self-Attention} $        | $ O(n^2 \cdot d) $          | $ O(1) $              | $ O(1) $             |
| $ \text{Recurrent} $             | $ O(n \cdot d^2) $          | $ O(n) $              | $ O(n) $             |
| $ \text{Convolutional} $         | $ O(k \cdot n \cdot d^2) $  | $ O(1) $              | $ O(\log(n)) $       |
| $ \text{Self-Attention (restricted)} $ | $ O(r \cdot n \cdot d) $ | $ O(1) $              | $ O(\frac{n}{r}) $   |

### 3.5. Positional Encoding

우리의 모델이 순환과 합성곱을 포함하지 않기 때문에, 모델이 시퀀스의 순서를 사용할 수 있도록 하기 위해, 우리는 시퀀스 내 토큰의 상대적 또는 절대적 위치에 대한 몇가지 정보를 주입해야 한다. 이를 위해, 우리는 인코더와 디코더 스택의 하단에 있는 입력 임베딩에 위치적 인코딩(positional encodings)을 추가한다. 위치적 인코딩은 동일한 차원 $d_{\text{model}}$를 가지고, 두 가지를 합칠 수 있다. 위치적 인코딩에는 학습된 것과 고정된 것 등 많은 선택이 있다.  

이 작업에서, 우리는 다른 주파수의 사인과 코사인 함수를 사용한다:  

$$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$
$$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$

여기서 $pos$는 위치이고, $i$는 차원이다. 즉, 위치적 인코딩의 각 차원은 사인파(sinusoid)에 해당한다. 파장은 $2\pi$ 에서 $10000 \cdot 2\pi$ 까지 기하학적인 진행을 형성한다. 우리는 이것이 모델이 상대적 위치에  주목하는 것을 더 쉽게 학습할 수 있도록 할 것이라고 가정했기 때문에 이 함수를 선택했고, 고정된 오프셋 $k$에 대해, $PE_{pos+k}$가 $PE_{pos}$의 선형 함수로 표현될 수 있기 때문이다.  

우리는 또한 학습된 위치적 임베딩을 사용하는 것으로 실험했고, 두 버전이 거의 동일한 결과를 생성한다는 것을 발견했다 (Table 3 row (E)). 우리는 이것이 모델이 학습 중에 마주친 것보다 긴 시퀀스의 길이를 추정할 수 있게 하기 때문에 사인파 버전을 선택했다.  

## 4. Why Self-Attention

이 섹션에서 $x_i, z_i \in \mathbb{R}^d$와 함께, 일반적인 시퀀스 변환 인코더 또는 디코더에서 은닉 층과 같이, 우리는 심볼 표현 시퀀스 $(x_1, \cdots, x_n)$를 동일한 길이의 다른 시퀀스 $(z_1, \cdots, z_n)$와 한 가변 길이를 매핑하기 위해 일반적으로 사용되는 재귀와 합성곱 레이어의 셀프 어텐션 레이어의 다양한 측면을 비교한다. 셀프 어텐션 사용의 동기를 부여하기 위해 우리는 세가지 희망 사항을 고려한다.  

하나는 레이어 별 전체 계산 복잡도이다. 다른 하나는 필요한 순차적인 작업의 최소값에 의해 측정되는 병렬화될 수 있는 계산의 양이다.  

세번째는 네트워크의 장거리 의존성 사이의 경로 길이이다. 장거리 의존성을 학습하는 것은 많은 시퀀스 변환 작업에서 중요한 도전 과제이다. 이러한 의존성을 학습하는 능력에 영향을 미치는 주요 요소 한가지는 네트워크에서 forward와 backward 신호가 순회 해야 하는(have to traverse) 경로의 길이이다. 입력과 출력 시퀀스의 위치 조합 사이의 이 경로가 짧을 수록, 장거리 의존성을 학습하는 것이 더 쉽다. 그러므로 우리는 또한 다른 레이어 종류로 구성된 네트워크에서 두 입력과 출력 위치 사이의 최대 경로 길이를 비교한다.  

Table 1에서 언급한 바와 같이, 셀프 어텐션 레이어는 순차적으로 실행된 연산의 일정한 수의 모든 위치를 연결하는 반면, 재귀 레이어는 $O(n)$의 순차적인 연산을 필요로 한다. 계산 복잡성 측면에서, 셀프 어텐션 레이어는 word-piece, byte-pair와 같이 기계 번역 SOTA 모델에 사용된 문장 표현의 대부분의 경우인 시퀀스 길이 $n$이 가장 표현 차원성 $d$보다 작을 때 재귀 레이어보다 빠르다. 매우 긴 시퀀스와 관련된 작업에서 계산 성능을 향상시키기 위해, 셀프 어텐션은 각 출력 위치를 중심으로 입력 시퀀스의 크기가 $r$인 이웃만을 고려하는 것을  제한할 수 있다. 이것은 최대 경로 길이를 $O(n/r)$ 로 증가시킨다. 우리는 향후 작업에서 이 접근을 더 조사할 계획이다.  

커널 폭이 $k<n$인 단일 합성곱 레이어는 입력과 출력 위치의 모든 쌍을 연결하지 않는다. 이를 수행하기 위해 연속 커널의 경우에는 $O(n/k)$의 합성곱 레이어의 스택이 필요하거나, 확장된 합성곱의 경우에는 $O(log_k(n))$가 필요하다. 합성곱 레이어는 순회 레이어보다 보통 $k$배 더 비싸다. 그러나 분리 가능한 합성곱은 복잡성을 $O(k \cdot n \cdot d + n \cdot d^2)$로 상당히 감소시킨다. 그러나 심지어 $k=n$ 분리 가능한 합성곱의 복잡성은 우리가 채택한 우리의 모델의 접근법인 셀프 어텐션 레이어와 지점 별 피드 포워드 레이어의 조합과 같다.  

부수적인 이점으로, 셀프 어텐션은 더 해석 가능한 모델을 생산할 수 있다. 우리는 우리 모델로부터 어텐션 분포를 검사하고 부록에서 예제를 논의한다. 개별적인 어텐션 헤드가 명백하게 다른 작업을 수행하도록 학습시킬 뿐만 아니라, 많은 것들이 문장의 구문적이고 의미적인 구조와 관련된 행동을 보이는 것으로 나타난다.  

## 5. Training

이 섹션은 우리 모델을 위한 훈련 체제를 설명한다.  

### 5.1. Training Data and Batching

우리는 약 450만 문장 쌍으로 구성된 표준 WMT 2014 영어-독일어 데이터셋으로 훈련했다. 문장들은 약 37000 토큰의 공유된 입력-출력(source-target) 단어들을 가진 byte-pair 인코딩을 사용하여 인코딩됐다. 영어-프랑스어에서, 우리는 훨씬 더 큰 36M의 문장들로 구성된  WMT 2014 영어-프랑스어 데이터셋을 사용했고, 32000개의 word-piece 단어들로 토큰을 나누었다. 문장 쌍은 대략적인 문장 길이로 묶어 배치되었다. 각 학습 배치는 약 25000개의 소스 토큰과 25000개의 타겟 토큰을 포함하는 문장 쌍 세트를 포함한다.  

### 5.2 Hardware and Schedule

우리는 NVIDIA P100 GPU 8개가 탑재된 한 대의 기계에서 우리의 모델을 훈련시킨다. 논문 전반에 설명된 하이퍼파라미터를 사용하는 우리의 베이스 모델의 경우, 각 훈련 단계는 약 0.4초가 걸린다. 우리는 총 10만 스텝(steps) 또는 12시간 동안 베이스 모델을 훈련했다. 우리의 큰 모델(table 3 하단에 설명된)의 경우, 스텝 시간은 1.0초였다. 큰 모델들은 30만 스텝(3.5일) 학습됐다.  

### 5.3 Optimizer

우리는 $\beta_1 = 0.9$, $\beta_2 = 0.98$ 그리고 $\varepsilon = 10^{-9}$로 설정하여 Adam 최적화 알고리즘(optimizer)을 사용했다. 우리는 공식에 따라 전반적인 훈련 과정에서 학습률을 변형시켰다:  

$$
\text{lrate} = d_{\text{model}}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})
$$

이것은 첫 $warmup_steps$ 훈련 스텝 이후 학습률을 선형적으로 증가시키고, 그 후에 그것을 스텝 수의 역제곱근에 비례적으로 감소시키는 것에 해당한다. 우리는 $warmup_steps = 4000$을 사용했다.  

### 5.4 Regularization

우리는 훈련 중에 3가지 정규화를 사용했다:  

Table 2: 트랜스포머는 훈련 비용의 일부만으로 영어-독일어와 영어-프랑스어 newstest2014 테스트에서 이전 SOTA 모델들보다 더 나은 BLEU 점수를 달성했다.  

| Model | BLEU EN-DE | BLEU EN-FR | Training Cost (FLOPs) EN-DE | Training Cost (FLOPs) EN-FR |
|-------|------------|------------|------------------------------|------------------------------|
| $ \text{ByteNet [18]} $       | $ 23.75 $    |              |                         |                         |
| $ \text{Deep-Att + PosUnk [39]} $ |          | $ 39.2 $     |                         | $ 1.0 \cdot 10^{20} $   |
| $ \text{GNMT + RL [38]} $     | $ 24.6 $     | $ 39.92 $    | $ 2.3 \cdot 10^{19} $   | $ 1.4 \cdot 10^{20} $   |
| $ \text{ConvS2S [9]} $        | $ 25.16 $    | $ 40.46 $    | $ 9.6 \cdot 10^{18} $   | $ 1.5 \cdot 10^{20} $   |
| $ \text{MoE [32]} $           | $ 26.03 $    | $ 40.56 $    | $ 2.0 \cdot 10^{19} $   | $ 1.2 \cdot 10^{20} $   |
| $ \text{Deep-Att + PosUnk Ensemble [39]} $ | | $ 40.4 $     |                         | $ 8.0 \cdot 10^{20} $   |
| $ \text{GNMT + RL Ensemble [38]} $ | $ 26.30 $ | $ 41.16 $    | $ 1.8 \cdot 10^{20} $   | $ 1.1 \cdot 10^{21} $   |
| $ \text{ConvS2S Ensemble [9]} $ | $ 26.36 $   | $ \mathbf{41.29} $    | $ 7.7 \cdot 10^{19} $   | $ 1.2 \cdot 10^{21} $   |
| $ \text{Transformer (base model)} $ | $ 27.3 $ | $ 38.1 $     | $ 3.3 \cdot 10^{18} $   ||
| $ \text{Transformer (big)} $  | $ 28.4 $     | $ 41.8 $     | $ 2.3 \cdot 10^{19} $     ||

**Residual Dropout:** 우리는 서브 레이어의 입력이 더해지고 정규화되기 전에 각 서브 레이어의 출력에 드롭아웃을 적용한다. 게다가, 우리는 인코더와 디코더 스택 둘 다의 임베딩의 합과 위치적 인코딩에 드롭아웃을 적용한다. 베이스 모델의 경우, 우리는 $P_{drop} = 0.1$의 비율을 사용한다.  

**Label Smoothing:** 훈련 동안, 우리는 값 $\xi_{ls} = 0.1$의 라벨 스무딩을 적용했다. 이것은 모델이 더 불확실해지는 것을 학습함에 따라 혼란도를 증가시킬 수 있지만, 정확도와 BLEU 점수를 향상시킨다.  