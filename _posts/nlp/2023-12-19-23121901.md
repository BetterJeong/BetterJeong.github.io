---
title: "[논문리뷰] Attention Is All You Need 정리"
excerpt: "어텐션(Attention) 기법과 트랜스포머(Transformer)"

categories:
  - NLP

toc: false
toc_sticky: false

date: 2023-12-19
last_modified_at: 2023-12-19
---

> '[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)' 논문을 한글로 번역한 내용은 [여기](https://betterjeong.github.io/nlp/23120501/)에서 확인하실 수 있습니다.

# The Transformer

트랜스포머(Transformer)는 "Attention is all you need"에서 처음으로 제안한 모델이다.  

트랜스포머는 seq2seq(sequence-to-sequence) 처럼 인코더-디코더 구조를 따른다.  
그러나 보통의 시퀀스 모델들은 RNN을 기반으로 하는데, 트랜스포머는 어텐션(Attention) 메커니즘만을 사용했다.  
기존 모델보다 훨씬 적은 훈련 비용으로 효율적인 병렬 처리가 가능하다는 특징이 있다.  

현재 BERT, GPT와 같은 트랜스포머 기반의 언어 모델이 계속해서 등장하고 있다.  

## Attention Mechanism

어텐션 메커니즘은 이 논문에서 처음으로 제시된 것은 아니다.  

seq2seq 모델과 같이 기존 RNN 기반의 시퀀스 모델에는 문제점이 있었다.  
seq2seq 모델은 입력 시퀀스를 고정된 벡터로 압축하여 출력 시퀀스를 만들어낸다.  
고정 길이의 벡터로 압축할 때, 시퀀스의 길이가 길어질 수록 정보 손실이 발생하게 된다.  
어텐션은 이때 시퀀스의 정확도가 떨어지지 않도록 보정하기 위해 사용됐다.  

트랜스포머는 이 어텐션만으로 인코더와 디코더를 구현한다.  

## 모델 구조

![Figure1](/assets/images/23120501/Figure1.png)  

seq2seq 처럼 인코더-디코더 형식으로 되어 있다.  

