---
title: "[밑바닥2] 5장 순환 신경망(RNN)"
excerpt: "확률과 언어 모델, RNN, RNNLM"

categories:
  - NLP

toc: false
toc_sticky: false

date: 2024-01-18
last_modified_at: 2024-01-18
---

# 순환 신경망 (RNN)

## 확률과 언어 모델

### word2vec을 확률 관점에서 바라보다

$w_1, w_2, \cdots , w_T$ 라는 단어열로 표현되는 코퍼스에서, CBOW 모델은 컨텍스트의 단어로부터 타깃 단어를 추측한다.  
$w_{t-1}$ 과 $w_{t+1}$ 이 주어졌을 때 타깃이 $w_t$ 가 될 확률은 아래와 같다.  

$$
{P(w_t \mid w_{t-1}, w_{t+1})}
$$

CBOW 모델은 위 식의 사후 확률을 모델링한다.  
사후 확률이란 $w_{t-1}$ 과 $w_{t+1}$ 이 주어졌을 때 $w_t$ 가 일어날 확률이다.  

컨텍스트가 좌우 대칭이 아닌 경우, 예를 들어 왼쪽 두 단어만 컨텍스트로 생각하는 경우 CBOW 모델의 확률은 아래와 같이 달라진다.  

$$
{P(w_t \mid w_{t-2}, w_{t-1})}
$$

위 식을 이용해 CBOW 모델이 다루는 손실 함수를 교차 엔트로피 오차에 의해 아래와 같이 유도할 수 있다.  

$$
L = - \log P(w_t \mid w_{t-2}, w_{t-1})
$$

### 언어 모델

언어 모델(Language Model)은 단어 나열에 확률을 부여한다.  
기계 번역, 음성 인식, 새로운 문장을 생성하는 용도로 이용된다.  

언어 모델을 수식으로 설명할 수 있다.  
$w_1, \cdots , w_m$ 이라는 $m$ 개의 단어로 된 문장이 있다고 할 때, 이 순서로 단어가 출현할 확률은 $P(w_1, \cdots , w_m)$ 로 나타낸다.  
이 확률은 여러 사건이 동시에 일어날 확률이므로 동시 확률이라고 한다.  

동시 확률 $P(w_1, \cdots , w_m)$ 은 사후 확률을 사용하여 아래와 같이 분해해서 쓸 수 있다.  

$$
\begin{matrix}
P(w_1, \cdots, w_m) &=& P(w_m | w_1, \cdots, w_{m-1})P(w_{m-1} | w_1, \cdots, w_{m-2}) \\
&& \cdots P(w_3 | w_1, w_2)P(w_2 | w_1)P(w_1) \\
&=& \prod_{i=1}^{m}P(w_i | w_1, \cdots, w_{i-1})
\end{matrix}
$$

동시 확률은 사후 확률의 총곱으로 나타낼 수 있다.  
이 결과는 확률의 곱셈정리로부터 유도할 수 있다.  

$$
P(A,B) = P(A\mid B)P(B)
$$

이는 A와 B가 모두 일어날 확률 $P(A, B)$ 가 B가 일어날 확률 $P(B)$와 B가 일어난 후 A가 일어날 확률 $P(A \mid B)$ 를 곱한 값과 같다는 것이다.  

이 곱셈 정리를 사용해 $m$ 개 단어의 동시 확률을 사후 확률로 나타낼 수 있게 된다.  
목적으로 하는 동시 확률은 사후 확률의 총곱인 $ \prod_{i=1}^{m}P(w_i \mid w_1, \cdots, w_{i-1}) $ 로 대표된다.  
이 사후 확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 컨텍스트로 했을 때의 확률이다.  
따라서 $ P(w_t \mid w_1, w_2, \cdots, w_{t-1}) $ 을 구하면 언어 모델의 동시 확률 $P(w_1, \cdots, w_m)$ 를 구할 수 있다.  

### CBOW 모델을 언어 모델로?

CBOW 모델을 언어 모델에 적용하려면 컨텍스트의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있다.  

$$
P(w_1, \ldots, w_m) = \prod_{i=1}^{m} P(w_i | w_1, \ldots, w_{i-1}) \approx \prod_{i=1}^{m} P(w_i | w_{i-2}, w_{i-1})
$$

여기서는 컨텍스트를 왼쪽 2개의 단어로 한정한다.  
직전 2개의 단어에만 의존해 다음 단어가 정해지는 모델이므로 '2층 마르코프 연쇄' 라고 부를 수 있다.  
마르코프 연쇄(Markov Chain) 또는 마르코프 모델(Markov Model)은 미래의 상태가 현재 상태에만 의존해 결정되는 것을 말한다.  

컨텍스트가 특정 길이로 고정되므로 더 왼쪽에 있는 단어는 무시되므로 문제가 발생할 수 있다. 
CBOW 모델의 컨텍스트를 얼마든지 키울 수 있지만, CBOW 모델에서는 맥락 안의 단어 순서가 무시된다.  

따라서 컨텍스트의 단어 순서를 고려한 모델이 필요하다.  
컨텍스트의 단어 벡터를 은닉층에서 연결(concatenate)하는 방식을 생각할 수 있다.  
신경 확률론적 언어 모델(Neural Probabilistic Language Model)에서 제안한 방식은 이 방식을 취한다.  
그러나 연결하는 방식을 취하면 컨텍스트의 크기에 비례해 가중치 매개변수도 늘어난다.  

이를 해결하기 위해 순환 신경망(RNN)이 등장한다.  
RNN은 컨텍스트가 아무리 길어도 정보를 기억하는 메커니즘을 갖추고 있다.  

## RNN이란

### 순환하는 신경망

순환하기 위해서는 닫힌 경로가 필요하다.  
닫힌 경로 혹은 순환하는 경로가 존재해야 데이터가 같은 장소를 반복해 왕래할 수 있다.  
RNN의 특징은 순환하는 경로가 있다는 것이다.  

![rnn](/assets/images/24011803/rnn.png)

순환 경로를 따라 데이터를 계층 안에서 순환시킨다.  
$\mathrm{x}_t$ 를 입력받는데, $t$ 는 시각을 뜻한다.  
이는 시계열 데이터 $(\mathrm{x_0, x_1, \cdots , x_t , \cdots})$ 가 RNN 계층에 입력됨을 표현한 것이다.  
그리고 입력에 대응하여 $(\mathrm{h_0, h_1, \cdots , h_t , \cdots})$ 가 출력된다.  

각 시각에 입력되는 $\mathrm{x}_t$ 는 벡터라고 가정한다.  
이 벡터가 순서대로 하나씩 RNN 계층에 입력된다.  

위 그림에서 출력이 2개로 분기되는데, 같은 것이 복제되어 하나는 자기 자신에 입력되는 것이다.  

### 순환 구조 펼치기

![rnn2](/assets/images/24011803/rnn2.png)

RNN 계층의 순환 구조를 피드포워드 신경망처럼 펼쳐서 표현할 수 있다.  
각 시각의 RNN 계층은 그 계층으로의 입력과 1개 전의 RNN 계층으로부터의 출력을 받는다.  
이 두 정보를 바탕으로 현 시각의 출력을 계산한다.  

$$
\mathrm{h}_t = \tanh (\mathrm{h}_{t-1} \mathrm{W_h+x}_t \mathrm{W_x+b})
$$

RNN에는 입력 $\mathrm{x}$ 를 출력 $\mathrm{h}$ 로 변환하기 위한 가중치 $\mathrm{W_x}$ 와 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 $\mathrm{W_h}$ , 편향 $b$ 가 있다.  
$\mathrm{h}_{t-1}$ 과 $\mathrm{x}_t$ 는 행벡터이다.  

이 식에서는 행렬 곱을 계산하고 그 합을 tanh 함수(쌍곡탄젠트 함수)를 이용해 변환한다.  

RNN은 $\mathrm{h}$ 라는 상태를 가지고 있고, 위 식의 형태로 갱신된다고 해석할 수 있다.  
따라서 RNN 계층을 상태를 가지는 계층 혹은 메모리(기억력)가 있는 계층이라고 한다.  

$\mathrm{h}_t$ 를 은닉 상태(hidden state) 혹은 은닉 상태 벡터(hidden state vector)라고 한다.  

### BPTT

![rnn3](/assets/images/24011803/rnn3.png)

먼저 순전파를 수행하고 이어서 역전파를 수행해서 원하는 기울기를 구하는 오차역전파법을 적용할 수 있다.  
여기서의 오차역전파법은 시간 방향으로 펼친 신경망의 오차역전파법이란 뜻으로 BPTT(Backpropagation Through Time)라고 한다.  

매 시각 RNN 계층의 중간 데이터를 메모리에 유지해두어야 하므로 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원이 늘어나는 문제가 있다.  
또한, 시간 크기가 커지면 역전파 시 기울기가 불안정해지는 문제도 있다.  

### Truncated BPTT

Truncated BPTT는 시간축 방향으로 길어진 신경망을 적당한 지점에서 잘라내어 작은 신경망 여러개로 만들어 오차역전파법을 수행하는 방식이다.  
역전파의 연결을 적당한 길이로 잘라내고 잘라낸 신경망 단위로 학습을 수행한다.  
RNN에서 Truncated BPTT를 수행할 때는 데이터를 순서대로 입력해야 한다.

![bptt](/assets/images/24011803/bptt.png)

첫 번째 블록 입력 데이터 $(\mathrm{x_0, \cdots , x_9})$ 를 RNN 계층에 제공한 것이다.  
순전파를 먼저 수행하고 역전파를 수행한다.  
이 과정을 통해 원하는 기울기를 얻을 수 있다.  

그리고 이어서 다음 블록의 입력 데이터$(\mathrm{x_{10}, \cdots , x_{19}})$를 입력해 오차역전파법을 수행한다.  

![bptt2](/assets/images/24011803/bptt2.png)

첫 번째 블록과 마찬가지로 순전파를 수행한 뒤 역전파를 수행하지만, 앞 블록의 마지막 은닉 상태인 $\mathrm{h_9}$ 가 필요하다.  

### Truncated BPTT의 미니배치 학습

미니배치 학습을 수행하기 위해서는 데이터를 주는 시작 위치를 각 미니배치의 시작 위치로 옮겨줘야 한다.  
길이가 1,000인 시계열 데이터에 대해 미니배치의 수가 2일 경우, RNN 계층의 입력 데이터로 첫 번째 미니배치 때는 처음부터 순서대로 데이터를 제공하고, 두 번째 미니배치 때는 500번째의 데이터를 시작 위치로 정하고, 그 위치부터 다시 순서대로 데이터를 제공한다.  

![bptt3](/assets/images/24011803/bptt3.png)

이처럼 미니배치 학습을 수행할 때는 각 미니배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 제공하면 된다.  
데이터를 순서대로 입력하다가 끝에 도달하면 다시 처음부터 입력한다.  
