---
title: "[PyTorch] 사전 학습된 모델(Pre-trained Model)"
excerpt: "백본과 다양한 전이 학습 유형, 특징 추출과 파인 튜닝"

categories:
  - AI

toc: false
toc_sticky: false

date: 2025-01-07
last_modified_at: 2025-01-07
---

> AI 스터디를 하며 '파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습' 교재를 정리한 글입니다.  

# 사전 학습된 모델(Pre-trained Model)

대규모 데이터셋으로 학습된 딥러닝 모델로, 이미 학습이 완료된 모델  
모델 자체를 시스템에 적용하거나 사전 학습된 임베딩(Embeddings) 벡터를 활용할 수 있음  
처음부터 모델을 구성하고 학습하지 않고 이미 학습된 모델의 일부를 활용하거나 추가 학습으로 모델 성능을 끌어낼 수 있음  

## 백본(Backbone)

합성곱 신경망 모델 VGG(Very Deep Convolutional Networks for Large-Scale Image Recognition), ResNet(Deep Residual Learning for Image Recognition), Mask R-CNN 논문 등에서 언급  
자연어처리, 컴퓨터비전 작업에서 백본이 되는 모델은 BERT, GPT, VGG-16, ResNet 등 초대규모 딥러닝 모델 사용  

입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 모델의 일부  
합성곱 계층이 입력 이미지를 고차원 특징 벡터로 변환해 이미지 분류 작업을 돕는 특징 추출기 역할로 사용  
노이즈와 불필요한 특성을 제거하고 중요한 특징 추출 가능, 추출한 특징으로 새로운 모델이나 기능의 입력으로 사용  

이미지에서 객체 검출 시  
- 초기 계층(하위 계층)에서 점, 선 등 저수준 특징 학습
- 중간 계층에서 객체나 형태 학습
- 최종 계층(상위 계층)에서 이전 계층의 특징 기반으로 객체 이해, 검출

포즈 추정(Pose Estimation) 모델, 이미지 분할(Image Segmentation) 모델로 확장 시 합성곱 신경망의 특징값으로 최종 계층을 바꿔 기존과 다른 모델 구성 가능  

## 전이 학습(Transfer Learning)

이미 사전 학습된 모델을 재사용해 새로운 작업, 관련 도메인 성능을 향상시키는 기술  
특정 영역 대규모 데이터셋에서 사전 학습된 모델을 다른 영역의 작은 데이터셋으로 파인 튜닝해 활용  
전 학습된 모델을 활용해 현재 시스템에 맞는 새로운 모델로 학습하는 과정  

업스트림(Upstream): 전이 학습 파이프라인 시작 부분으로 대규모 특정 도메인 데이터셋에서 학습한 사전 학습된 모델  
다운스트림(Downstream): 전이 학습 파이프라인 마지막 부분으로 업스트링 모델에 작은 규모의 타깃 도메인 데이터셋으로 파인 튜닝된 모델  

| 유형        | 세부 유형    | 소스 도메인 레이블링 | 타깃 도메인 레이블링 |
|-------------|---------------|-------------|-------------|
| 귀납적 전이 학습 | 자기주도적 학습 | 없음   | 있음   |
| 귀납적 전이 학습 | 다중 작업 학습 | 있음   | 없음   |
| 변환적 전이 학습 | -             | 있음   | 없음   |
| 비지도 전이 학습 | -             | 없음   | 없음   |
| 제로-샷 전이 학습| -             | 있음   | 없음   |
| 원-샷 전이 학습  | -             | 있음   | 없음   |

### 귀납적 전이 학습(Inductive Transfer Learning)

이전 작업에서 학습한 지식을 새로운 작업에 활용  

1. 자기주도적 학습(Self-taught Learning)
  - 비지도 전이 학습(Unsupervised Transfer Learning) 유형 중에 하나  
  - 소스 도메인의 데이터셋의 데이터 양이 많지만 레이블링 데이터 수가 매우 적거나 없을 때 사용
  - 오토 인코더 등 레이블이 지정되지 않은 대규모 데이터셋에서 특징을 추출하는 모델을 학습시켜 저차원 공간에서 레이블링된 데이터로 파인 튜닝  
2. 다중 작업 학습(Multi-task Learning)
  - 레이블이 지정된 소스 도메인, 타겟 도메인 데이터 기반으로 여러 작업을 동시에 가르치는 방법
  - 공유 계층(Shared Layers), 작업별 계층(Task Spectific Layer)로 나뉨
    - 공유 계층: 소스 도메인, 타깃 도메인 데이터셋에서 모델을 사전 학습  
    - 작업별 계층: 단일 작업을 위해 계층마다 타깃 도메인 데이터셋으로 파인 튜닝, 작업마다 다른 학습 데이터셋을 사용함  

### 변환적 전이 학습(Transductive Transfer Learning)

소스 도메인과 타깃 도메인이 유사하지만 동일하지 않은 경우  
레이블이 지정된 소스 도메인으로 사전 학습 모델을 구축하고 레이블이 없는 타깃 도메인으로 파인 튜닝  

1. 도메인 적응(Domain Adaptation)
   - 서로 다른 도메인의 특징 공간과 분포를 고려해 학습  
   - 도메인 변화(Domain Shift)를 확인하여 전이
   - 타깃 도메인에서 성능 향상이 목적이므로 소스 도메인이 조정될 수 있음  
2. 표본 선택 편향/공변량 이동(Sample Selection Bias/Covariance Shift)
   - 소스 도메인과 타깃 도메인의 분산과 편향이 크게 다를 때 표본을 선택하여 편향, 공변량 이동
   - 소스 도메인, 타깃 도메인이 완전히 동일하지 않으므로 학습 데이터보다 테스트 데이터에서 성능이 좋지 않을 수 있음  
   - 무작위/비무작위 샘플링 방법, 도메인 적응을 통해 해당 학습치만 전이시킴  

### 비지도 전이 학습(Unsupervised Transfer Learning)

소스 도메인, 타깃 도메인 모두 레이블이 없을 경우의 전이 학습 방법  
레이블이 없는 전체 데이터로 학습하여 데이터의 특징, 특성을 구분할 수 있게 사전 학습된 모델 구축  
소규모의 레이블링된 데이터로 파인 튜닝하여 타깃 도메인에 대한 모델 초기화  

레이블 영향을 받지 않고 특징을 학습하여 파인 튜닝 시 더 효과적으로 예측 수행 가능  
작은 데이터셋으로도 우수한 결과를 얻을 수 있음  

자연어처리, 컴퓨터비전 등 다양한 분야에서 사용, 데이터 부족 문제 극복 가능  
대표적 방법으로 생성적 적대 신경망(Generative Adversarial Networks, GAN), 군집화(Clustering)가 있음  
자연어처리에서는 Word2Vec, fastText, BERT, 컴퓨터비전에서는 ResNet-50, VGG-16등이 있음  

### 제로-샷 전이 학습(Zero-shot Transfer Learning)

이전에 본 적 없는 개체나 개념을 인식할 수 있도록 하는 학습 방법  

### 원-샷 전이 학습(One-shot Transfer Learning)

제로샷과 유사하지만 한 번에 하나의 샘플만 사용해 모델 학습  
매우 적은 양의 데이터로 분류 문제 해결 가능  
서포트셋, 쿼리셋을 가정해 서포트셋의 대표 샘플과 쿼리 셋 간의 거리를 측정해 쿼리 셋과 가장 가까운 서포트셋의 대표 샘플 클래스로 분류  
유클리드 거리, 코사인 유사도 등 활용  

서포트셋(Support Set): 학습에 사용될 클래스의 대표 샘플  
쿼리 셋(Query Set): 새로운 클래스를 분류하기 위한 입력 데이터(분류 대상 데이터)  

## 특징 추출(Feature Exraction) 및 미세 조정(Fine-tuning)

전이 학습에 사용되는 일반적인 기술  
대규모 데이터셋으로 학습된 모델을 작은 데이터셋으로 추가 학습해 가중치와 편향 수정  

### 특징 추출

타깃 도메인이 소스 도메인과 유사하고 타깃 도메인의 데이터셋이 적을 때 사용  
타깃 도메인으로 학습해도 소스 도메인의 가중치나 편향이 유사하기 때문에 특징 추출 계층을 동결(Freeze)해 학습하지 않고 기존 학습된 모델의 가중치를 사용함  
예측 모델마다 요구하는 출력 모델 수가 다르므로 분류기(Classifier)만 재구성해 학습  

### 미세 조정

특징 추출 계층을 일부만 변경하거나 동결하지 않고 타깃 도메인에 대한 학습 진행  
데이터의 개수와 유사성에 따라 전략이 달라짐  

- 데이터가 많고 유사도가 낮은 경우: 분류기를 포함한 모델 매개변수 다시 학습  
- 데이터가 적고 유사도도 낮은 경우: 초기 계층의 저수준 특징 추출 기능 동결 후 나머지 계층과 분류기 학습  
- 데이터가 많고 유사도도 높은 경우: 상위 계층과 분류기 학습  
