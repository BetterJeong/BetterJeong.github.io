---
title: "[PyTorch] 순전파와 역전파"
excerpt: "순전파, 역전파 계산하기"

categories:
  - AI

toc: false
toc_sticky: false

date: 2024-12-31
last_modified_at: 2024-12-31
---

> AI 스터디를 하며 '파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터비전 심층학습' 교재를 정리한 글입니다.  

# 순전파(Forward Propagation)와 역전파(Back Propagation)

- 순전파(Forward Propagation): 순방향 전달(Forward Pass)이라고도 함, 입력이 주어지면 출력을 계산하는 프로세스

$$
\hat{y} = activation(weight \times x + bias)
$$

- 역전파(Back Propagation): 순전파 방향과 반대로 연산 진행, 학습 과정에서 순전파 과정에서 나온 오차를 활용해 가중치와 편향을 최적화하기 위해 연쇄법칙 활용  

```python
import torch
import torch.nn as nn
import torch.optim as optim
```

```python
class CustomModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Sequential(
            nn.Linear(2, 2),
            nn.Sigmoid()
        )
        self.layer2 = nn.Sequential(  # 두 개의 계층
            nn.Linear(2, 1),
            nn.Sigmoid()
        )

        self.layer1[0].weight.data = torch.nn.Parameter(
            torch.Tensor([[0.4352, 0.3545],
                          [0.1951, 0.4835]])
        )
        self.layer1[0].bias.data = torch.nn.Parameter(
            torch.Tensor([-0.1419, 0.0439])
        )
        self.layer2[0].weight.data = torch.nn.Parameter(
            torch.Tensor([[-0.1725, 0.1129]])
        )
        self.layer2[0].bias.data = torch.nn.Parameter(
            torch.Tensor([-0.3043])
        )
```

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CustomModel().to(device)
criterion = nn.BCELoss().to(device)  # 이진 교차 엔트로피
optimizer = optim.SGD(model.parameters(), lr=1)  # 확률적 경사 하강법
```

## 순전파 계산

1. 첫 번째 계층의 가중합(Weighted Sum) 계산

$z$는 입력값($x$)과 가중치($W$)의 곱을 모두 더한 값에 편향($b$)을 더한 값인 가중합이고, 계산하려는 값은 $z1$, $z2$  

$$
z_1 = W_1 x_1 + W_2 x_2 + b_1 = 0.4352 \times 1 + 0.3545 \times 1 - 0.1419 = 0.6478
$$

$$
z_2 = W_3 x_1 + W_4 x_2 + b_2 = 0.1951 \times 1 + 0.4835 \times 1 - 0.0439 = 0.7225
$$

2. 가중합에 활성화 함수 적용

시그모이드 함수로 첫 번째 계층의 가중합 활성화  

$$
\sigma_1 = \frac{1}{1 + e^{-z_1}} = \frac{1}{1 + e^{-0.6478}} = 0.6565
$$

$$
\sigma_2 = \frac{1}{1 + e^{-z_2}} = \frac{1}{1 + e^{-0.7225}} = 0.6732
$$

3. 두 번째 계층의 가중합과 가중합 활성화

$\sigma$를 입력값으로 사용해 동일한 방식으로 두 번째 계층 가중합에 시그모이드 적용  

$$
z_3 = W_5 \sigma_1 + W_6 \sigma_2 + b_3 = -0.1725 \times 0.6565 + 0.1129 \times 0.6731 - 0.3043 = -0.3415
$$

$$
\sigma_3 = \frac{1}{1 + e^{-z_3}} = \frac{1}{1 + e^{0.3415}} = 0.4154
$$

`output = model(x)` 가 순전파 과정이며, $\sigma_3$가 `output` 변수에 할당됨  
최종으로 계산된 $\sigma_3$의 값이 예측값($\hat{y}$)  

## 오차 계산

이진 교차 엔트로피를 사용했으므로 실젯값($y=0$)과 예측값($\hat{y}=0.4154$)으로 오차 계산  
파이토치 코드로는 `loss = criterion(output, y)` 에 해당  
$\mathcal{L}$ 값이 `loss` 변수가 됨  

$$
\begin{align*}
\mathcal{L} &= -(y \log(\hat{y}) + (1-y) \log(1-\hat{y})) \\
&= -(0 \log(0.4154) + (1-0) \log(1-0.4154)) \\
&= -\log(0.5846) \\
&= 0.5368
\end{align*}
$$

## 역전파 계산

계층의 역순으로 가중치와 편향 갱신  
모델의 학습이 오차가 작아지는 방향으로 갱신되어야 하므로 미분값이 0에 가까워져야 함  
$W_5, W_6, b_3$을 갱신하고 $W_1, W_2, W_3, W_4, b_1, b_2$가 갱신됨  

갱신된 가중치와 편향의 기울기는 오차가 0이 되는 방향으로 진행하므로, 갱신된 가중치나 편향은 기울기를 감산해 변화가 없을 때까지 반복  

$$
W_n(t+1) = W_n(t) - \alpha \frac{\partial \mathcal{L}}{\partial W_n(t)}
$$

$t$: 가중치 갱신 횟수  
$\alpha$: 학습률  

이 식으로 가중치를 계속 갱신하면 $\alpha \frac{\partial \mathcal{L}}{\partial W_n(t)}$ 이 점점 0에 가까워짐  

$W_5, W_6, b_3$부터 $W_1, W_2, W_3, W_4, b_1, b_2$를 모두 갱신하면 학습이 1회 진행된 것으로 볼 수 있음  
갱신된 가중치와 편향으로 다음 학습을 진행하고, 진행할수록 오차가 점차 감소하게 됨  
배치 크기가 1보다 크다면 행렬 계산으로 진행  