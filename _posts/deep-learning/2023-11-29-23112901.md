---
title: "[밑바닥2] 2장 자연어와 단어의 분산 표현"
excerpt: "시소러스, 통계 기반 기법과 개선 방법"

categories:
  - Deep Learning

toc: false
toc_sticky: false

date: 2023-11-29
last_modified_at: 2023-11-29
---

# 자연어와 단어의 분산 표현

## 자연어 처리란

자연어(natural language)란 한국어, 영어 등 인간이 평소에 사용하는 말이다.  
같은 의미의 문장도 여러 형태로 표현할 수 있고, 문장의 뜻이 애매할 수 있으며, 의미나 형태가 유연하게 바뀐다.  

자연어 처리(Natural Language Processing)는 자연어를 처리하는 분야로, 우리의 말을 컴퓨터에게 이해시키기 위한 기술(분야)이라고 할 수 있다.  

컴퓨터에게 자연어를 이해시키려는 노력의 예로 검색 엔진, 기계 번역, 질의응답 시스템, IME(입력기 전환), 문장 자동 요약과 감정분석 등이 있다.  

## 시소러스

시소러스는 유의어 사전으로, 뜻이 같은 단어(동의어)나 뜻이 비슷한 단어(유의어)가 한 그룹으로 분류되어 있다.  
자연어처리에 이용되는 시소러스는 단어 사이의 상위와 하위, 전체와 부분 등 더 세세한 관계까지 정의해둔 경우도 있다.  
유의어 집합을 만들어 단어들의 관계를 그래프로 표현해서 단어 사이의 연결을 정의하고, 이 단어 네트워크를 이용해 컴퓨터에게 단어 사이의 관계를 가르칠 수 있다.  

### WordNet

자연어 처리 분야에서 가장 유명한 시소러스이다.  
프린스턴 대학교에서 1985년부터 구축하기 시작한 전통 있는 시소러스이다.  
이를 통해 유의어를 얻거나 단어 네트워크를 이용할 수 있다.  

### 시소러스의 문제점

사람이 수작업으로 레이블링하는 방식에는 결정이 존재한다.  

- 시대 변화에 대응하기 어렵다.  
우리가 사용하는 말은 때때로 새로운 단어가 생겨나고 옛말은 잊혀진다.  
시대에 따라 언어의 의미가 변하기도 한다.  
이러한 변화에 대응하기 위해서 시소러스를 사람이 수작업으로 끊임없이 갱신해야 한다.  

- 사람을 쓰는 비용이 크다.  
방대한 단어들 모두에 대해 단어 사이의 관계를 정의해야 하므로 엄청난 인적 비용을 발생시킨다.  

- 단어의 미묘한 차이를 표현할 수 없다.  
빈티지와 레트로같이 의미가 같지만 용법이 다른 단어들의 미묘한 차이를 표현할 수 없다.  

오랫동안 특징을 수동적으로 설계하였으나, 딥러닝을 통해 사람의 개입을 줄일 수 있게 되었다.  
사람의 개입을 최소로 줄이고 텍스트 데이터만으로 원하는 결과를 얻어내는 방향으로 패러다임이 바뀌고 있다.  

## 통계 기반 기법

대량의 텍스트 데이터인 말뭉치(corpus)를 사용한다.  
자연어 처리에 사용되는 코퍼스에는 텍스트 데이터에 대한 추가 정보가 포함되는 경우가 있다.  
예를 들어 텍스트 데이터의 단어 각각에 품사를 레이블링 할 수 있다.  
보통은 컴퓨터가 다루기 쉬운 트리 구조 등의 형태로 가공되어 주어진다.  

### 파이썬으로 코퍼스 전처리하기

```python
text = 'You say goodbye and I say hello.'
```

```python
text = text.lower()
text = text.replace('.', ' .')
text
```

```
'you say goodbye i say hello .'
```

```python
words = text.split(' ')
words
```

```
['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']
```

문장 첫머리의 대문자로 시작하는 단어를 처리하기 위해 `lower()` 메서드를 사용해 모든 문자를 소문자로 변환한다.  
`split('')` 메서드를 통해 공백 기준으로 분할한다.  
마침표 구분을 위해 마침표 앞에 공백을 추가했다.  

정규표현식을 통해 간단하게 단어 단위로 분할할 수 있다.  

```python
word_to_id = {}
id_to_word = {}

for word in words:
    if word not in word_to_id:
        new_id = len(word_to_id)
        word_to_id[word] = new_id
        id_to_word[new_id] = word
```

단어에 ID를 부여하고 ID의 리스트로 이용할 수 있도록 한 번 더 손질하는 과정이다.  
`id_to_word` 를 통해 단어 ID에서 단어로 변환하고, `word_to_id` 를 통해 단어에서 단어 ID로 변환한다.  
`words` 의 단어를 하나씩 살펴보면서 `word_to_id` 에 들어있지 않으면 각 배열에 새로운 ID와 단어를 추가한다.

딕셔너리를 사용해 단어를 가지고 단어 ID를 검색하거나 단어 ID로 단어를 검색할 수 있게 된다.  

```python
id_to_word[1]
word_to_id['hello']
```

```
'say'
5
```

`words` 를 단어 ID 목록으로 변경한다.  

```python
import numpy as np
corpus = [word_to_id[w] for w in words]
corpus = np.array(corpus)
corpus
```

```
array([0, 1, 2, 3, 4, 1, 5, 6])
```

전처리를 위한 `preprocess()` 함수를 구현해보자.  

```python
def preprocess(text):
    text = text.lower()
    text = text.replace('.', ' .')
    words = text.split(' ')

    word_to_id = {}
    id_to_word = {}

    for word in words:
        if word not in word_to_id:
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word

    return corpus, word_to_id, id_to_word
```

```python
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
```

### 단어의 분산 표현

단어의 의미를 정확하게 파악할 수 있도록 벡터로 표현하는 것을 단어의 분산 표현(distributional representation)이라고 한다.  
단어를 고정 길이의 밀집 벡터(dense vector)로 표현하고, 밀집 벡터는 대부분의 원소가 0이 아닌 실수인 벡터를 말한다.  
예를 들어 3차원의 분산 표현은 [0.21, -0.45, 0.83]과 같은 모습이다.  

### 분포 가설

단어의 의미는 주변 단어에 의해 형성된다는 것을 분포 가설(distributional hypothesis)이라고 한다.  
단어 자체에는 의미가 없고, 그 단어가 사용된 맥락(context)이 의미를 형성한다는 것이다.  
단어를 벡터로 표현하는 최근 연구도 대부분 이 가설에 기초한다.  

예를 들어 "I dring beer"와 "We drink wine"처럼 "drink"의 주변에는 음료가 등장하기 쉬울 것이다.  
"I guzzle beer"와 "We guzzle wine"이라는 문장이 있다면, "guzzle"은 "drink"와 같은 맥락에서 사용된다는 것을 알 수 있다.  
그리고 "guzzle"과 "drink"가 가까운 의미의 단어라는 것을 알 수 있다.  

컨텍스트는 특정 단어를 중심에 둔 그 주변 단어를 말한다.  
컨텍스트의 크기(주변 단어를 몇 개나 포함할지)를 윈도우 크기(window size)라고 한다.  
윈도우 크기가 1이면 좌우 한 단어씩, 2이면 좌우 두 단어씩 맥락에 포함된다.  
상황에 따라 왼쪽 단어만, 오른쪽 단어만 사용할 수도 있고, 문장의 시작과 끝을 고려할 수도 있다.  

### 동시발생 행렬 (co-occurrence matrix)

어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법을 통계 기반 기법이라고 한다.  

```python
import sys
sys.path.append('..')
import numpy as np
from common.util import preprocess

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

print(corpus)
# [0 1 2 3 4 1 5 6]

print(id_to_word)
# [0: 'you', 1: 'say', 2:'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.']
```

각 단어의 맥락에 포함되는 단어의 빈도를 표로 정리한다.  
이 표가 행렬의 형태를 띤다는 뜻에서 동시발생 행렬이라고 한다.  
표의 각 행은 해당 단어를 표현한 벡터가 된다.  

위의 동시발생 행렬을 파이썬으로 구현하면 아래와 같다.  

```python
C = np.array([
    [0, 1, 0, 0, 0, 0, 0],
    [1, 0, 1, 0, 1, 1, 0],
    [0, 1, 0, 1, 0, 0, 0],
    [0, 0, 1, 0, 1, 0, 0],
    [0, 1, 0, 1, 0, 0, 0],
    [0, 1, 0, 0, 0, 0, 1],
    [0, 0, 0, 0, 0, 1, 0],
], dtype=np.int32)
```

```python
print(C[0])
# [0 1 0 0 0 0 0]
```

이와 같이 동시발생 행렬을 활용하면 단어를 벡터로 나타낼 수 있다.  

코퍼스로부터 동시발생 행렬을 만들어주는 함수를 구현하여 동시발생 행렬의 구현을 자동화할 수 있다.  

```python
def create_co_matrix(corpus, vocab_size, window_size=1):
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

    for idx, word_id in enumerate(corpus):
        for i in range(1, window_size + 1):
            left_idx = idx - i
            right_idx = idx + i

            if left_idx >= 0:
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] += 1

            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1

    return co_matrix
```

인수는 차례로 단어 ID 리스트, 어휘 수, 윈도우 크기를 나타낸다.  
0으로 채워진 2차원 배열을 생성하고, 코퍼스의 모든 단어 각각에 대해 윈도우에 포함된 주변 단어를 센다.  
코퍼스의 끝 경계를 벗어나지 않는지도 확인한다.  

### 벡터 간 유사도

대표적으로 벡터의 내적이나 유클리드 거리 등을 꼽을 수 있다.  
단어 벡터 사이의 유사도를 나타낼 때는 코사인 유사도를 자주 이용한다. 
코사인 유사도는 두 벡터가 가리키는 방향이 얼마나 비슷한가를 의미한다.  
두 벡터의 방향이 완전히 같다면 코사인 유사도가 1이 되고, 완전히 반대라면 -1이 된다.  

두 벡터 $\mathbf{x}=(x_1, x_2, x_3, \cdots, x_n)$ 과 $\mathbf{y}=(y_1, y_2, y_3, \cdots, y_n)$ 이 있다면 코사인 유사도는 다음 식으로 정의된다.  

$$
\mathrm{similiarity}(\mathbf{x}, \mathbf{y}) = 
\frac{\mathbf{x} \cdot \mathbf{y}}{\begin{Vmatrix} \mathbf{x} \end{Vmatrix} {\begin{Vmatrix} \mathbf{y} \end{Vmatrix}}} = 
\frac{x_1y_1 + \cdots + x_ny_n}
{\sqrt{x_1^2 + \cdots + x^2_n}
\sqrt{y^2_1 + \cdots + y^2_n}}
$$

분자에는 벡터의 내적, 분모에는 각 벡터의 노름(norm)이 등장한다.  
노름은 벡터의 크기를 나타낸 것으로, 여기서는 벡터의 각 원소를 제곱해 더한 후 다시 제곱근을 구해 계산하는 L2 노름을 계산한다.  

