---
title: "[밑바닥2] 1장 신경망 정리"
excerpt: "수학과 파이썬, 신경망의 추론, 학습"

categories:
  - Deep Learning

toc: false
toc_sticky: false

date: 2023-11-15
last_modified_at: 2023-11-15
---

# 신경망 복습

## 수학과 파이썬 복습

### 벡터와 행렬

벡터는 크기와 방향을 가진 양으로, 숫자가 일렬로 늘어선 집합으로 표현할 수 있다.  
파이썬에서는 1차원 배열로 취급한다.  
숫자를 세로로 나열하면 열벡터, 가로로 나열하면 행벡터이다.  

행렬은 숫자가 2차원 형태(사각형 형상)으로 늘어선 것으로, 2차원 배열로 표현할 수 있다.  
가로줄은 행(row), 세로줄은 열(column)이라 한다.  
벡터와 행렬을 확장하여 숫자 집합을 N차원으로 표현한 것을 텐서라고 한다.  

```python
import numpy as np
```

행렬을 취급하기 위해 넘파이를 사용한다.  

```python
x = np.array([1, 2, 3])
x.__class__
x.shape
x.ndim
```

행렬은 `np.array()` 메서드로 생성할 수 있다.  
이 메서드는 넘파이의 다차원 배열 클래스인 `np.ndarray` 클래스를 생성한다.  
`nparray` 에는 다양한 편의 메서드와 인스턴스 변수가 있다.  
`shape` 는 다차원 배열 형상, `ndim` 은 차원 수를 담고 있다.  

```python
<class 'numpy.ndarray'>
(3,)
1
```

x는 1차원 배열이며 원소 수가 3개인 벡터임을 알 수 있다.  

```python
W = np.array([[1, 2, 3], [4, 5, 6]])
W.shape
W.ndim
```

```python
(2, 3)
2
```

W는 2차원 배열이며, 2×3 행렬임을 알 수 있다.  

### 원소별 (element-wise) 연산

```python
W = np.array([[1, 2, 3], [4, 5, 6]])
X = np.array([[0, 1, 2], [3, 4, 5]])
```

```python
X + Y
```

```python
array([[1, 3, 5],
       [7, 9, 11]])
```

```python
X * Y
```

```python
array([[0, 2, 6],
       [12, 20, 30]])
```

피연산자인 다차원 배열들에서 서로 대응하는 원소끼리(각 원소가 독립적으로) 연산이 이뤄진다.  

### 브로드캐스트 (broadcast)

넘파이의 다차원 배열에서는 형상이 다른 배열끼리도 연산할 수 있다.  

```python
A = np.array([[1, 2],
              [3, 4]])
A * 10
```

```python
array([[10, 20],
       [30, 40]])
```

행렬 A에 10이라는 스칼라 값을 곱하게 되면, 스칼라 값 10이 2×2 행렬로 확장된 후 원소별 연산을 수행한다.  
이러한 기능을 브로드캐스트라고 한다.  

```python
A = np.array([[1, 2],
              [3, 4]])
b = np.array([10, 20])
A * b
```

```python
array([[10, 40],
       [30, 80]])
```

이 계산에서는 1차원 배열인 b가 2차원 배열 A와 형상이 같아지도록 확장된다.  

![broadcast](/assets/images/23111501/broadcast_ex_2.jpg)  

### 벡터의 내적

$x{\cdot}y=x_1y_1+x_2y_2+{\cdots}+x_ny_n$

2개의 벡터 $x = (x_1, {\cdots}, x_n)$, $y=(y_1, {\cdots},y_n)$가 있다고 가정할 때, 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것이다.  

```python
a = np.array([1, 2, 3])
b = np.array([1, 2, 3])
np.dot(a, b)
```

```python
32
```

넘파이의 `np.dot()` 으로 쉽게 구할 수 있다.  

### 행렬의 곱

![matrix_multiplication](/assets/images/23111501/matrix_multiplication.jpg)  

행렬의 곱은 왼쪽 행렬의 행벡터(가로 방향)와 오른쪽 행렬의 열벡터(세로 방향)의 내적으로 계산한다.  
계산 결과는 새로운 행렬의 대응하는 원소에 저장된다.  
예를 들어 A의 1행과 B의 1열의 계산 결과는 1행 1열 위치의 원소가 된다.  

```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
np.matmul(A, B)
```

```python
array([[19, 22],
       [43, 50]])
```

넘파이의 `np.matmul()` 메서드로 쉽게 구할 수 있다.  

행렬의 곱에도 `np.dot()` 을 사용할 수 있다.  
인수가 모두 1차원 배열이면 벡터의 내적을 계산하고, 2차원 배열이면 행렬의 곱을 계산한다.  
그러나 가능하면 구분하여 코드의 논리와 의도를 명확히 하는 게 좋다.  

### 행렬 형상 확인

행렬이나 벡터를 계산할 때는 형상(shape)에 주의해야 한다.  

![geometry_check](/assets/images/23111501/geometry_check.jpg)  

행렬의 곱에서는 대응하는 차원의 원소 수를 일치시켜야 한다.  
3×2 행렬과 2×4 행렬을 곱하여 3×4 행렬을 만들 경우 위 그림처럼 대응하는 차원의 원소 수가 같아야 한다.  
결과로 만들어지는 행렬의 형상은 A의 행 수와 B의 열 수가 되고 이 과정을 형상 확인이라고 한다.  

## 신경망의 추론

신경망은 단순한 함수라고 할 수 있다.  
함수처럼 입력을 출력으로 변환한다.  

2차원 데이터를 입력하여 3차원 데이터를 출력하는 함수를 예로 들면, 이를 신경망으로 구현하려면 입력층(input layer)에는 뉴런 2개, 출력층(output layer)에는 3개를 각각 준비한다.  
은닉층(hidden layer)에도 적당한 수의 뉴런을 배치하는데, 뉴런 4개를 두기로 가정한다.  

![neural_network](/assets/images/23111501/neural_network.jpg)  

화살표에는 가중치(weight)가 존재하고, 가중치와 뉴런의 값을 각각 곱한 뒤, 그 합에 활성화 함수를 적용한 값이 다음 뉴런의 입력으로 쓰인다.  
각 층에서 편향(bias)이라고 하는 이전 뉴런의 값에 영향받지 않는 정수를 더한다.  

위 그림의 신경망처럼 인접하는 층의 모든 뉴런과 연결되어 있다면 완전연결계층(fully connected layer)이라고 한다.  

입력층의 데이터는 $(x_1,x_2), 가중치는 $w_{11}$과 $w_{21}$, 편향은 $b_1$로 표현할 때, 수식으로 나타내면 아래와 같다.  

$h_1 = x_1w_{11}+x_2w_{21}+b_1$  

이처럼 은닉층의 뉴런은 가중치의 합으로 계산된다.  
가중치와 편향의 값을 바꿔가며 뉴런 수만큼 반복하면 은닉층에 속한 모든 뉴런 값을 구할 수 있다.  

가중치와 편향에는 첨자(인덱스)가 붙는다.  
첨자는 가중치 합으로 계산되며, 행렬의 곱으로 한꺼번에 계산할 수 있다.  
단, 첨자를 붙이는 규칙은 중요하지 않다.  

완전연결계층이 수행하는 변환은 행렬의 곱을 이용해 아래처럼 정리해서 쓸 수 있다.  

$(h_1,h_2,h_3,h_4)=(x_1,x_2)\begin{pmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\ w_{21} & w_{22} & w_{23} & w_{24} \end{pmatrix}+(b_1,b_2,b_3,b_4)$  

$(h_1,h_2,h_3,h_4)$은 은닉층의 뉴런들이고, 1×4 행렬 혹은 행벡터이다.  
입력 $(x_1,x_2)$는 1×2 행렬이고, 가중치는 2×4 행렬, 편향은 1×4 행렬에 대응한다.  
간소화한 식은 아래와 같다.  

$h=xW+b$  

$x$는 입력, $h$는 은닉층의 뉴런, $W$는 가중치, $b$는 편향을 뜻한다.  
형상을 확인해보면 대응하는 차원의 원소 수가 일치한다.  

다수의 샘플 데이터(미니배치)를 처리하려면 행렬 $x$의 행 각각에 샘플 데이터를 하나씩 저장해야 한다.  

```python
import numpy as np
W1 = np.random.randn(2, 4)  # 가중치
b1 = np.random.randn(4)     # 편향
x = np.random.randn(10, 2)  # 입력
h = np.matmul(x, W1) + b1
```

10개의 샘플 데이터 각각을 완전연결계층으로 변환시키는 예제이다.  
코드 마지막 줄의 편향 b1의 덧셈은 브로드캐스트된다.  

완전연결계층에 의한 변환은 선형 변환이라고 하고, 활성화 함수를 통해 비선형 효과를 부여한다.  
비선형 활성화 함수를 이용함으로써 신경망의 표현력을 높일 수 있다.  

### 시그모이드 함수 (sigmoid function)

${\sigma}(x)={1\over1+\exp(-x)}$

시그모이드 함수는 알파벳 S자 모양의 곡선 함수이다.  

![sigmoid_function](/assets/images/23111501/sigmoid_function.jpg)

시그모이드 함수는 임의의 실수를 입력받아 0에서 1 사이의 실수를 출력한다.  

```python
def sigmoid(x):
       return 1 / (1 + np.exp(-x))
```

시그모이드 함수 식을 그대로 표현한 코드이다.  
이 코드를 통해 위의 은닉층 뉴런 h를 변환하면 아래와 같다.  

```python
a = sigmoid(h)
```

시그모이드 함수를 통해 비선형 변환한 출력 a를 활성화라고 한다.  
그리고 계속해서 또 다른 완전연결계층에 통과시켜 반환한다.  
위 예제의 은닉층의 뉴런은 4개, 출력층의 뉴런은 3개이므로 완전연결계층에 사용되는 가중치 행렬은 4×3 형상으로 설정해야 한다.  
이를 통해 출력층의 뉴런을 얻을 수 있다.  

### 예제 코드

```python
import numpy as np

def sigmoid(x):
       return 1 / (1 + np.exp(-x))

x = np.random.randn(10, 2)
W1 = np.random.randn(2, 4)
b1 = np.random.randn(4)
W2 = np.random.randn(4, 3)
b2 = np.random.randn(3)

h = np.matmul(x, W1) + b1
a = sigmoid(h)
s = np.matmul(a, W2) + b2
```

x의 형상이 (10, 2)이므로, 2차원 데이터 10개가 미니배치로 처리된다는 뜻이다.  
최종 출력인 s의 형상은 (10, 3)이고, 10개의 데이터가 한꺼번에 처리되었으며 3차원 데이터라는 의미이다.  

이 신경망은 3차원 데이터를 출력하므로 각 차원의 값을 이용해 3클래스 분류를 할 수 있다.  
출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 점수(score)가 된다.  
실제로 분류를 한다면 출력층에서 가장 큰 값을 출력하는 뉴런에 해당하는 클래스가 예측 결과가 된다.  

### 계층으로 클래스화 및 순전파 구현

신경망 추론 과정에서 하는 처리는 신경망의 순전파(forward propagation)에 해당한다.  
순전파는 입력층에서 출력층으로 향하는 전파이다.  
순전파 때는 신경망을 구성하는 각 계층이 입력으로부터 출력 방향으로 처리 결과를 차례로 전파한다.  
데이터를 순전파와 반대 방향으로 전파하는 경우 역전파(backward propagation)라고 한다.  

시그모이드 함수를 클래스로 구현한 Sigmoid 계층은 아래와 같다.  

```python
import numpy as np

class Sigmoid:
       def __init__(self):
              self.params = []

       def forward(self, x):
              return 1 / (1 + np.exp(-x)) 
```

순전파를 수행하는 `forward()` 메서드를 가지며, 가중치와 편향 등의 매개변수들은 params 인스턴스 변수에 보관한다.  
Sigmoid 계층에는 학습하는 매개변수가 없으므로 params는 빈 리스트로 초기화한다.  

완전연결계층에 의한 변환은 기하학에서의 아핀(affine) 변환에 해당하므로 `Affine` 계층이다.  
Affine 계층의 구현은 아래와 같다.  

```python
class Affine:
```