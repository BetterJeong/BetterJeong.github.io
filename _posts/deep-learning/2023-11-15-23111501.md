---
title: "[밑바닥2] 1장 신경망 정리"
excerpt: "수학과 파이썬, 신경망의 추론, 학습"

categories:
  - Deep Learning

toc: false
toc_sticky: false

date: 2023-11-15
last_modified_at: 2023-11-15
---

# 신경망 복습

## 수학과 파이썬 복습

### 벡터와 행렬

벡터는 크기와 방향을 가진 양으로, 숫자가 일렬로 늘어선 집합으로 표현할 수 있다.  
파이썬에서는 1차원 배열로 취급한다.  
숫자를 세로로 나열하면 열벡터, 가로로 나열하면 행벡터이다.  

행렬은 숫자가 2차원 형태(사각형 형상)으로 늘어선 것으로, 2차원 배열로 표현할 수 있다.  
가로줄은 행(row), 세로줄은 열(column)이라 한다.  
벡터와 행렬을 확장하여 숫자 집합을 N차원으로 표현한 것을 텐서라고 한다.  

```python
import numpy as np
```

행렬을 취급하기 위해 넘파이를 사용한다.  

```python
x = np.array([1, 2, 3])
x.__class__
x.shape
x.ndim
```

행렬은 `np.array()` 메서드로 생성할 수 있다.  
이 메서드는 넘파이의 다차원 배열 클래스인 `np.ndarray` 클래스를 생성한다.  
`nparray` 에는 다양한 편의 메서드와 인스턴스 변수가 있다.  
`shape` 는 다차원 배열 형상, `ndim` 은 차원 수를 담고 있다.  

```python
<class 'numpy.ndarray'>
(3,)
1
```

x는 1차원 배열이며 원소 수가 3개인 벡터임을 알 수 있다.  

```python
W = np.array([[1, 2, 3], [4, 5, 6]])
W.shape
W.ndim
```

```python
(2, 3)
2
```

W는 2차원 배열이며, 2×3 행렬임을 알 수 있다.  

### 원소별 (element-wise) 연산

```python
W = np.array([[1, 2, 3], [4, 5, 6]])
X = np.array([[0, 1, 2], [3, 4, 5]])
```

```python
X + Y
```

```python
array([[1, 3, 5],
       [7, 9, 11]])
```

```python
X * Y
```

```python
array([[0, 2, 6],
       [12, 20, 30]])
```

피연산자인 다차원 배열들에서 서로 대응하는 원소끼리(각 원소가 독립적으로) 연산이 이뤄진다.  

### 브로드캐스트 (broadcast)

넘파이의 다차원 배열에서는 형상이 다른 배열끼리도 연산할 수 있다.  

```python
A = np.array([[1, 2],
              [3, 4]])
A * 10
```

```python
array([[10, 20],
       [30, 40]])
```

행렬 A에 10이라는 스칼라 값을 곱하게 되면, 스칼라 값 10이 2×2 행렬로 확장된 후 원소별 연산을 수행한다.  
이러한 기능을 브로드캐스트라고 한다.  

```python
A = np.array([[1, 2],
              [3, 4]])
b = np.array([10, 20])
A * b
```

```python
array([[10, 40],
       [30, 80]])
```

이 계산에서는 1차원 배열인 b가 2차원 배열 A와 형상이 같아지도록 확장된다.  

![broadcast](/assets/images/23111501/broadcast_ex_2.jpg)  

### 벡터의 내적

$$x{\cdot}y=x_1y_1+x_2y_2+{\cdots}+x_ny_n$$

2개의 벡터 $x = (x_1, {\cdots}, x_n)$, $y=(y_1, {\cdots},y_n)$가 있다고 가정할 때, 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것이다.  

```python
a = np.array([1, 2, 3])
b = np.array([1, 2, 3])
np.dot(a, b)
```

```python
32
```

넘파이의 `np.dot()` 으로 쉽게 구할 수 있다.  

### 행렬의 곱

![matrix_multiplication](/assets/images/23111501/matrix_multiplication.jpg)  

행렬의 곱은 왼쪽 행렬의 행벡터(가로 방향)와 오른쪽 행렬의 열벡터(세로 방향)의 내적으로 계산한다.  
계산 결과는 새로운 행렬의 대응하는 원소에 저장된다.  
예를 들어 A의 1행과 B의 1열의 계산 결과는 1행 1열 위치의 원소가 된다.  

```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
np.matmul(A, B)
```

```python
array([[19, 22],
       [43, 50]])
```

넘파이의 `np.matmul()` 메서드로 쉽게 구할 수 있다.  

행렬의 곱에도 `np.dot()` 을 사용할 수 있다.  
인수가 모두 1차원 배열이면 벡터의 내적을 계산하고, 2차원 배열이면 행렬의 곱을 계산한다.  
그러나 가능하면 구분하여 코드의 논리와 의도를 명확히 하는 게 좋다.  

### 행렬 형상 확인

![geometry_check](/assets/images/23111501/geometry_check.jpg)  

행렬의 곱에서는 대응하는 차원의 원소 수를 일치시켜야 한다.  
결과로 만들어지는 행렬의 형상은 A의 행 수와 B의 열 수가 되고 이 과정을 형상 확인이라고 한다.  

## 신경망의 추론

신경망은 함수처럼 입력을 출력으로 변환한다.  

2차원 데이터를 입력하여 3차원 데이터를 출력하는 함수를 예로 들면, 이를 신경망으로 구현하려면 입력층(input layer)에는 뉴런 2개, 출력층(output layer)에는 3개를 각각 준비한다.  
은닉층(hidden layer)에도 적당한 수의 뉴런을 배치하는데, 뉴런 4개를 두기로 가정한다.  

![neural_network](/assets/images/23111501/neural_network.jpg)  

화살표에는 가중치(weight)가 존재하고, 가중치와 뉴런의 값을 각각 곱한 뒤, 그 합에 활성화 함수를 적용한 값이 다음 뉴런의 입력으로 쓰인다.  
각 층에서 편향(bias)이라고 하는 이전 뉴런의 값에 영향받지 않는 정수를 더한다.  

위 그림의 신경망처럼 인접하는 층의 모든 뉴런과 연결되어 있다면 완전연결계층(fully connected layer)이라고 한다.  

입력층의 데이터는 $(x_1,x_2), 가중치는 $w_{11}$과 $w_{21}$, 편향은 $b_1$로 표현할 때, 수식으로 나타내면 아래와 같다.  

$$h_1 = x_1w_{11}+x_2w_{21}+b_1$$  

이처럼 은닉층의 뉴런은 가중치의 합으로 계산된다.  
가중치와 편향의 값을 바꿔가며 뉴런 수만큼 반복하면 은닉층에 속한 모든 뉴런 값을 구할 수 있다.  

가중치와 편향에는 첨자(인덱스)가 붙는다.  
첨자는 가중치 합으로 계산되며, 행렬의 곱으로 한꺼번에 계산할 수 있다.  
단, 첨자를 붙이는 규칙은 중요하지 않다.  

완전연결계층이 수행하는 변환은 행렬의 곱을 이용해 아래처럼 정리해서 쓸 수 있다.  

$$(h_1,h_2,h_3,h_4)=(x_1,x_2)\begin{pmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\ w_{21} & w_{22} & w_{23} & w_{24} \end{pmatrix}+(b_1,b_2,b_3,b_4)$$  

$(h_1,h_2,h_3,h_4)$은 은닉층의 뉴런들이고, 1×4 행렬 혹은 행벡터이다.  
입력 $(x_1,x_2)$는 1×2 행렬이고, 가중치는 2×4 행렬, 편향은 1×4 행렬에 대응한다.  
간소화한 식은 아래와 같다.  

$$h=xW+b$$  

$x$는 입력, $h$는 은닉층의 뉴런, $W$는 가중치, $b$는 편향을 뜻한다.  
형상을 확인해보면 대응하는 차원의 원소 수가 일치한다.  

다수의 샘플 데이터(미니배치)를 처리하려면 행렬 $x$의 행 각각에 샘플 데이터를 하나씩 저장해야 한다.  

```python
import numpy as np
W1 = np.random.randn(2, 4)  # 가중치
b1 = np.random.randn(4)     # 편향
x = np.random.randn(10, 2)  # 입력
h = np.matmul(x, W1) + b1
```

10개의 샘플 데이터 각각을 완전연결계층으로 변환시키는 예제이다.  
코드 마지막 줄의 편향 b1의 덧셈은 브로드캐스트된다.  

완전연결계층에 의한 변환은 선형 변환이라고 하고, 활성화 함수를 통해 비선형 효과를 부여한다.  
비선형 활성화 함수를 이용함으로써 신경망의 표현력을 높일 수 있다.  

### 시그모이드 함수 (sigmoid function)

$${\sigma}(x)={1\over1+\exp(-x)}$$

시그모이드 함수는 알파벳 S자 모양의 곡선 함수이다.  

![sigmoid_function](/assets/images/23111501/sigmoid_function.jpg)

시그모이드 함수는 임의의 실수를 입력받아 0에서 1 사이의 실수를 출력한다.  

```python
def sigmoid(x):
       return 1 / (1 + np.exp(-x))
```

시그모이드 함수 식을 그대로 표현한 코드이다.  
이 코드를 통해 위의 은닉층 뉴런 h를 변환하면 아래와 같다.  

```python
a = sigmoid(h)
```

시그모이드 함수를 통해 비선형 변환한 출력 a를 활성화라고 한다.  
그리고 계속해서 또 다른 완전연결계층에 통과시켜 반환한다.  
위 예제의 은닉층의 뉴런은 4개, 출력층의 뉴런은 3개이므로 완전연결계층에 사용되는 가중치 행렬은 4×3 형상으로 설정해야 한다.  
이를 통해 출력층의 뉴런을 얻을 수 있다.  

### 예제 코드

```python
import numpy as np

def sigmoid(x):
       return 1 / (1 + np.exp(-x))

x = np.random.randn(10, 2)
W1 = np.random.randn(2, 4)
b1 = np.random.randn(4)
W2 = np.random.randn(4, 3)
b2 = np.random.randn(3)

h = np.matmul(x, W1) + b1
a = sigmoid(h)
s = np.matmul(a, W2) + b2
```

x의 형상이 (10, 2)이므로, 2차원 데이터 10개가 미니배치로 처리된다는 뜻이다.  
최종 출력인 s의 형상은 (10, 3)이고, 10개의 데이터가 한꺼번에 처리되었으며 3차원 데이터라는 의미이다.  

이 신경망은 3차원 데이터를 출력하므로 각 차원의 값을 이용해 3클래스 분류를 할 수 있다.  
출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 점수(score)가 된다.  
실제로 분류를 한다면 출력층에서 가장 큰 값을 출력하는 뉴런에 해당하는 클래스가 예측 결과가 된다.  

### 계층으로 클래스화 및 순전파 구현

신경망 추론 과정에서 하는 처리는 신경망의 순전파(forward propagation)에 해당한다.  
순전파는 입력층에서 출력층으로 향하는 전파이다.  
순전파 때는 신경망을 구성하는 각 계층이 입력으로부터 출력 방향으로 처리 결과를 차례로 전파한다.  
데이터를 순전파와 반대 방향으로 전파하는 경우 역전파(backward propagation)라고 한다.  

시그모이드 함수를 클래스로 구현한 Sigmoid 계층은 아래와 같다.  

```python
import numpy as np

class Sigmoid:
       def __init__(self):
              self.params = []

       def forward(self, x):
              return 1 / (1 + np.exp(-x)) 
```

순전파를 수행하는 `forward()` 메서드를 가지며, 가중치와 편향 등의 매개변수들은 params 인스턴스 변수에 보관한다.  
Sigmoid 계층에는 학습하는 매개변수가 없으므로 `params` 는 빈 리스트로 초기화한다.  

완전연결계층에 의한 변환은 기하학에서의 아핀(affine) 변환에 해당하므로 `Affine` 계층이다.  
Affine 계층의 구현은 아래와 같다.  

```python
class Affine:
       def __init__(self, W, b):
              self.params = [W, b]

       def forward(self, x):
              W, b = self.params
              out = np.matmul(x, W) + b
              return out
```

초기화될 때 가중치와 편향을 받는다.  
가중치와 편향은 Affine 계층의 매개변수로 신경망이 학습될 때 수시로 갱신되고, 리스트인 `params` 인스턴스 변수에 보관한다.  
`forward(x)` 는 순전파 처리를 구현한다.  

### 신경망 추론 처리 구현

![neural_network_inference_processing](/assets/images/23111501/neural_network_inference_processing.jpg)  

입력 $x$가 `Affine` 계층, `Sigmoid` 계층, `Affine` 계층을 차례로 거쳐 점수인 $s$를 출력하게 된다.  

```python
class TwoLayerNet:
       def __init__(self, input_size, hidden_size, output_size):
              I, H, O = input_size, hidden_size, output_size

              W1 = np.random.randn(I, H)
              b1 = np.random.randn(H)
              W2 = np.random.randn(H, O)
              b2 = np.random.randn(O)

              self.layers = [
                     Affine(W1, b1),
                     Sigmoid(),
                     Affine(W2, b2)
              ]

              self.params = []
              for layer in self.layers:
                     self.params += layer.params
              
       def predict(self, x):
              for layer in self.layers:
                     x = layer.forward(x)
              return x


x = np.random.randn(10, 2)
model = TwoLayerNet(2, 4, 3)
s = model.predict(x)
```

신경망을 `TwoLayerNet` 이라는 클래스로 추상화하고, 주 추론 처리는 `predict(x)` 메서드로 구현했다.  
초기화 메서드는 가중치를 초기화하고 3개의 계층을 생성한다.  
학습해야 할 가중치 매개변수들을 `params` 리스트에 저장한다.  

## 신경망의 학습

학습되지 않은 신경망은 좋은 추론을 할 수 없으므로 학습을 먼저 수행해야 한다.  
추론이란 다중 클래스 분류 등의 문제의 답을 구하는 작업이다.  

### 손실 함수

손실은 학습 데이터와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 단일 값(스칼라)이다.  
신경망의 손실은 손실 함수(loss function)를 사용해서 구한다.  

다중 클래스 분류(multi-class classification) 신경망에서는 손실 함수로 교차 엔트로피 오차(Cross Entropy Error)를 많이 사용한다.  
교차 엔트로피 오차는 신경망이 출력하는 각 클래스의 확률과 정답 레이블로 구할 수 있다.  

아래는 손실 함수를 적용한 신경망의 계층 구성이다.  

![neural_network_applying_loss_function](/assets/images/23111501/neural_network_applying_loss_function.jpg)

$x$는 입력 데이터, $t$는 정답 레이블, $L$은 손실을 나타낸다.  

`Softmax` 계층의 출력은 확률이 되고, 다음 계층인 `Cross Entropy Error` 계층에는 `Softmax` 계층에서 출력된 확률과 정답 레이블($t$)이 입력된다.  

소프트맥스 함수 식은 아래와 같다.  

$$y_k={\exp(S_k)\over\sum\limits_{i=1}\limits^{n}\exp(s_i)}$$

출력이 총 $n$개일 때, $k$번째의 출력 $y_k$를 구하는 계산식이다.  
$y_k$는 $k$번째 클래스에 해당하는 소프트맥스 함수의 출력이다.  
분자는 점수 $s_k$의 지수 함수이고, 분모는 모든 입력 신호의 지수 함수의 총합이다.  

소프트맥스 함수의 출력의 각 원소는 0.0 이상 1.0 이하의 실수이고, 그 원소들을 모두 더하면 1.0이 되므로 확률로 해석할 수 있다.  

교차 엔트로피 오차의 수식은 아래와 같다.  

$$L=-\sum\limits_{k}t_k\log{y_k}$$

$t_k$는 $k$번째 클래스의 정답 레이블이다.  
정답 레이블은 $t=[0,0,1]$과 같이 원핫 벡터로 표기한다.  
원핫 벡터(one-hot vector)란 1개의 원소만 1이고, 나머지는 0인 벡터이다.  
1인 원소가 정답 클래스에 해당한다.  
따라서 1의 원소에 해당하는 자연로그만 계산하고, 다른 원소들은 $t_k$가 0이므로 계산 결과에 영향을 주지 않는다.  

미니배치 처리를 고려한 교차 엔트로피 오차 식은 아래와 같다.  

$$L=-{1\over{N}}\sum\limits_{n}\sum\limits_{k}t_{nk}\log{y_{nk}}$$

데이터는 $N$개이고, $t_{nk}$는 $n$번째 데이터의 $k$차원째의 값을 의미한다.  
$y_{nk}$는 신경망의 출력이고, $t_{nk}$는 정답 레이블이다.  
$N$으로 나눠서 1개당의 평균 손실 함수를 구하여 미니배치의 크기의 관계 없이 항상 일관된 척도를 얻을 수 있다.  

### 미분과 기울기

여러 개의 변수(다변수)도 미분할 수 있다.  

벡터의 각 원소에 대한 미분을 정리한 것이 기울기(gradient)이다.  

$L$은 스칼라, $x$는 벡터인 함수 $L=f(x)$가 있다면, $x_i$에 대한 $L$의 미분은 ${\delta}L\over{\delta}x_i$로 쓸 수 있다.  
이를 정리하면 아래와 같다.  

$${{\delta}L\over{\delta}x}=\begin{pmatrix}{{\delta}L\over{\delta}x_1}, {{\delta}L\over{\delta}x_2}, \cdots,{{\delta}L\over{\delta}x_n}\end{pmatrix}$$  

$W$가 $m\times{n}$행렬에서 $L=g(W)$ 함수의 기울기는 아래와 같이 구할 수 있다.  

$${{\delta}L\over{\delta}W}=\begin{pmatrix}{{\delta}L\over{\delta}W_{11}}&&\cdots&&{{\delta}L\over{\delta}W_{1n}}\\\vdots&&\ddots\\{{\delta}L\over{\delta}W_{m1}}&&&&{{\delta}L\over{\delta}W_{mn}}\end{pmatrix}$$

$W$와 ${{\delta}L\over{\delta}W}$의 형상은 같다.  
행렬과 그 기울기의 형상이 같다는 성질을 이용하여 매개변수 갱신과 연쇄 법칙을 쉽게 구현할 수 있다.  

### 연쇄 법칙 (chain rule)

연쇄 법칙이란 합성함수에 대한 미분 법칙이다.  
$y=f(x)$와 $z=g(y)$라는 두 함수가 있을 때, $z=g(f(x))$가 되고, 이 합성함수의 미분은 아래와 같이 구한다.  

$${{\delta}z\over{\delta}x}={{\delta}z\over{\delta}y}{{\delta}y\over{\delta}x}$$

우리가 다루는 함수가 아무리 복잡하다 하더라도, 그 미분은 개별 함수의 미분을 이용해 구할 수 있기 때문에 연쇄법칙이 중요하다.  
각 함수의 국소적인 미분을 계산할 수 있으면 그 값들을 곱해서 전체의 미분을 구할 수 있다.  

### 계산 그래프

계산 그래프는 계산 과정을 시각적으로 보여준다.  

아래는 $z=x+y$를 나타낸 계산 그래프이다.  

![simple_graph](/assets/images/23111501/simple_graph.jpg)

이 계산의 앞뒤로 어떤 계산이 있다고 가정하고, 최종적으로 스칼라 값인 $L$이 출력된다고 가정한다.  

![add_node](/assets/images/23111501/add_node.jpg)

$L$의 미분을 각 변수에 대해 구하고자 할 때, 역전파는 아래와 같이 그릴 수 있다.  

![backpropagation](/assets/images/23111501/backpropagation.jpg)

연쇄 법칙에 따르면 ${{\delta}L\over{\delta}x}={{\delta}L\over{\delta}z}{{\delta}z\over{\delta}x}$이고, ${{\delta}L\over{\delta}y}={{\delta}L\over{\delta}z}{{\delta}z\over{\delta}y}$이다.  

